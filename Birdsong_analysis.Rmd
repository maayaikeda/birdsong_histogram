---
title: "Birdsong analysis"
output: rmarkdown::github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This is an analysis of songs recorded from two zebra finches. Zebra finche males sing to their mates during courtship. Previous studies have found that males sing shorter (faster) songs to their mates (called Directed song) than they are practicing alone (Undirected songs).

Here, I have audio files from males recorded while they were with their mates. Noramally, we would know whether the songs are directed or undirected from their behavior such as the direction they are facing and whether or not they are dancing to their mate. However, there are no videos recorded for this data.


### Question and hypothesis
Then, how do I figure out which ones are "directed" and which ones are "undirected" songs?

I have noticed that some songs are sang while the females are making calls. There are some indications from previous studies that females like to respond to directed songs with calls.

**I hypothesize that the majority of songs recordings that are contaminated with female calls are directed songs and the others are undirected. **

If this is true, I expect songs with female calls to be shorter/faster than songs with no calls.


## Preparing Data

Load and prepare data.


```{r data}
library(readxl)

birdsong_1 <- read_excel("~/Documents/Home 2020/birdsong_1.xlsx")
birdsong_2 <- read_excel("~/Documents/Home 2020/birdsong_2.xls")

# Combine data
data <- rbind(birdsong_1, birdsong_2)

# clean up data; get rid of durations that are too short to be songs 
new_data<- data[400 < data$motif_duration,]
birdsong2_filtered<- birdsong_2[400 < birdsong_2$motif_duration,]
```

## Preparing data/preliminary plot for Bird #2

For bird2 (O375), songs were recorded when he was with his mate and offspring, so some song audio files contain his mate's calls and his offsprings begging. I expect offspring begging to not make a difference.

```{r plot}
library(ggplot2)
p <- ggplot(birdsong2_filtered, aes(motif_duration,fill=context2)) + geom_histogram(binwidth = 2, , alpha = 0.5, position = "identity") + ggtitle("Motif duration: context")+ xlab("Motif duration") +labs(fill = "Sounds in Recording")
p <- p + scale_fill_brewer(palette="Set2")
# get rid of grid
p <- p + theme_classic()
# label y axis
p <- p + ylab("Count")
p

```

As expected, songs sang during begging calls do not seem to be shorter in duration than males singing to themselves, so I decided to categorize it as as male singing alone. 

```{r rename}

new_data[new_data == "alone"]<-"Male singing alone"
new_data[new_data == "interact"]<-"Mate responding to song"
new_data[new_data == "beg"]<-"Male singing alone"

```

For bird1 (G544), songs were recorded while he was completely alone and when he was with a female. The songs while he was separated from his mate can be safely assumed to be "directed".

Plotting data to see how it looks.

## Preliminary plot 2 to see how the overall distributions are for both birds

```{r testplot}
# plot data and see how it looks
p <- ggplot(new_data, aes(motif_duration,fill=context2, color=birdID)) + geom_histogram(binwidth = 2, , alpha = 0.5, position = "identity") + ggtitle("Motif duration: context")+ xlab("Motif duration") +labs(fill = "Sounds in Recording")
p <- p + scale_fill_brewer(palette="Set2")
# get rid of grid
p <- p + theme_classic()
# label y axis
p <- p + ylab("Count")
p

```

It does look like the songs sung when females are calling are shorter in duration than the songs male are singing to themselves. 

Since these two birds have different song duration, I am normalizing them to their "undirected" song or solo song recordings. 

## Summarizing data

Looking at data summary and mean durations to normalize data.
```{r prepfornormalization}


# Need this library to summarize data. Load it.
library(Rmisc)
# Summary table. Call it df1.
df1 <- summarySE(new_data, measurevar = "motif_duration", groupvars=c("birdID", "context2"))
df1
```

## Normalizing data to the mean

Taking the difference from the mean.

```{r normalization}
# Count number of rows 
rows = 1:length(new_data$motif_duration)
# Create empty column
new_data[,"diff_frm_mean"] <- NA

# Extract means for undirected (alone) songs
G544undir_mean <- df1[1, 4]
O375undir_mean <- df1[3, 4]


# Add motif_duration-mean values in the new column. 
# If the bird is G544, substract G544 undirected mean, otherwise, substract O375's undirected mean.
for (r in rows){
  if (new_data$birdID[r] == "G544"){
    new_data$diff_frm_mean[r] <- new_data$motif_duration[r]-G544undir_mean
    } else { 
      new_data$diff_frm_mean[r] <- new_data$motif_duration[r]-O375undir_mean
  }
}


```


## Plot normalized grouped data

```{r plot2}
library(ggplot2)
p <- ggplot(new_data, aes(diff_frm_mean,color=context2)) + geom_histogram(fill = "white",position = "dodge") + ggtitle("Motif length")
p <- p + labs(color = "Context") + xlab("Motif duration difference from mean")
p <- p + theme_classic()
p



```

## Plot density plot for grouped data
Density plot.

```{r plot3}
# Another way to summarize mean.
library(plyr)
mu2 <- ddply(new_data, "context2", summarise, grp.mean=mean(diff_frm_mean))
head(mu2)

# Plot density
p <- ggplot(new_data, aes(x=diff_frm_mean, color=context2, fill=context2))
p <- p + geom_histogram(aes(y=..density..), position="identity", alpha=0.5) 
p <- p + geom_density(alpha=0.5)
p <- p + geom_vline(data=mu2, aes(xintercept=grp.mean, color=context2),linetype="dashed")
p <- p + scale_color_manual(values=c("pink3", "darkseagreen3"),name="Experimental\nCondition",
                            breaks=c("Male singing alone", "Mate responding to song"),
                     labels=c("Suspected Undirected", "Suspected Directed"))
p <- p + scale_fill_manual(values=c("pink3", "darkseagreen3"),name="Experimental\nCondition",
                    breaks=c("Male singing alone", "Mate responding to song"),
                    labels=c("Suspected Undirected", "Suspected Directed"))
p <- p + labs(title="Motif Length", y = "Density", x="Motif length difference from directed mean")
p <- p + theme_classic()
p



```

Overall, for grouped data, the "undirected" songs (songs while male singing alone)

## Run t-test
```{r ttest}
S_Undirected <- subset(new_data == "Male singing alone")
S_Directed <-subset(new_data == "Mate responding to song")

t.test(S_Undirected,S_Directed)

```





## Plot by bird

Plot by bird to see if this is the the case for ungrouped data.
But first, rename the data so that it's clearer. 

```{r renaming}

new_data[,"contexts"] <- NA
for (r in rows){
  if (new_data$birdID[r] == "G544" && new_data$context2[r] == "Male singing alone"){
    new_data$contexts[r] <- "Undirected G544"
  } else if (new_data$birdID[r] == "G544" && new_data$context2[r] == "Mate responding to song"){
    new_data$contexts[r] <- "Female calls in recording G544"
  } else if (new_data$birdID[r] == "O375" && new_data$context2[r] == "Mate responding to song"){
    new_data$contexts[r] <- "Female calls in recording O375"
  } else { 
    new_data$contexts[r] <- "Male singing alone O375"
  }
}


# Get a stats summary for density plot.
df2 <- summarySE(new_data, measurevar = "diff_frm_mean", groupvars=c("birdID", "contexts"))
df2




```

```{R plotbybird}

# Density plot for each bird. Pink = G544, Green = O375; lighter color = undirected
# lighdarker color = directed
p <- ggplot(new_data, aes(x=diff_frm_mean, color=contexts, fill=contexts))
p <- p + geom_histogram(aes(y=..density..), position="identity", alpha=0.5) 
p <- p + geom_density(alpha=0.5)
p <- p + geom_vline(data=df2, aes(xintercept=diff_frm_mean, color=contexts),linetype="dashed")
p <- p + scale_color_manual(values = c("pink3", "pink1", "darkseagreen3", "darkseagreen1"), name = "Context/Bird",
                            breaks = c("Female calls in recording G544", "Undirected G544", "Female calls in recording O375", "Male singing alone O375"),
                            labels = c("Suspected directed G544", "Undirected G544", "Suspected directed O375", "Suspected undirected O375"))
p <- p + scale_fill_manual(values = c("pink3", "pink1", "darkseagreen3", "darkseagreen1"), name = "Context/Bird",
                            breaks = c("Female calls in recording G544", "Undirected G544", "Female calls in recording O375", "Male singing alone O375"),
                            labels = c("Suspected directed G544", "Undirected G544", "Suspected directed O375", "Suspected undirected O375"))
p <- p + labs(title="Undirected vs Directed in birds G544 and O375", y = "Density", x="Motif length difference from directed mean")
p <- p + annotate("segment", x = 0, y = 0, xend = 0, yend =0.065 )
p <- p + annotate("text", x = 5,y = 0.07, label = "Undirected mean" )
p <- p + theme_classic()
p
```
## Run stats by bird
```{r birdstats}
# Run anova
anov <- aov(new_data$diff_frm_mean ~ new_data$contexts)
# output summary
summary(anov)
# Run tukey test for pairwise comparison
TukeyHSD(anov)
```

As expected, songs with and without female calls in the background are significantly different in duration.

## Conclusion

1. For bird #1 (G544), the songs that he sang while his mate was calling were overall shorter in duration than his undirected song. 
2. For bird #2 (O375), while we don't have recordings from him while he was alone, his song recording whithout the contamination of female calls were longer in duration than the songs sang while his mate was calling. 

This suggest that we presence of female call contamination could be a way to sort undirected vs. directed songs when visual analysis of behavior cannot be done. 

## Using Machine learning algorithm


Using Machine learning algorithm to see how different are the songs during different contexts for each bird. 

```{r dataforknn}
library(caret)
library(e1071)
birdsong1 <- subset(new_data, birdID == "G544")
index <- createDataPartition(birdsong1$contexts, p=0.75, list=FALSE)

# the data needs to be in a data frame or it wont work
birdsong1 <- as.data.frame(birdsong1)

# convert contexts column to factor
birdsong1$contexts <- as.factor(birdsong1$contexts)

data.training <- birdsong1[index,c(2:16,27,28)]
data.test <- birdsong1[-index,c(2:16,27,28)]

data.test <- as.data.frame(data.test )
```

Using KNN model.

```{r knnmodel}

# Train model

model_knn <- train(data.training[,1:16], data.training$contexts, method = 'knn',preProcess=c("center", "scale") )
predictions <- predict(object = model_knn, data.test[,1:16], type="raw")

resultstest <- data.test[,17]
confusionMatrix(predictions,resultstest)


```


Looking at which variables contributed the most. We expect motif duration to be ranked high.

```{r importance}

importance <- varImp(model_knn, scale=FALSE)
print(importance)
```

As expected, motif duration is the most important variable other than start time. Mean entropy is also expected to be different  between the two groups because isolated recordings are clean while recordings while females are calling are messy (less clean) by definition.

## Plot data


Plot data to see how well the top two variables separate out the two contexts.
```{r scatterplot}


# plot scatter

p <- ggplot(birdsong1, aes(x = motif_duration, y = mean_entropy, color=contexts))
p <- p + geom_point()
p <- p +  theme_classic()
p

```

## Repeat for Bird #2

```{r dataforknnb2}

birdsong2 <- subset(new_data, birdID == "O375")
index <- createDataPartition(birdsong2$contexts, p=0.75, list=FALSE)

# the data needs to be in a data frame or it wont work
birdsong2 <- as.data.frame(birdsong2)

# convert contexts column to factor
birdsong2$contexts <- as.factor(birdsong2$contexts)

data.training <- birdsong2[index,c(2:16,27,28)]
data.test <- birdsong2[-index,c(2:16,27,28)]

data.test <- as.data.frame(data.test )
```

Using KNN model to start.

```{r knnmodelb2}

# Train model

model_knn <- train(data.training[,1:16], data.training$contexts, method = 'knn',preProcess=c("center", "scale") )
predictions <- predict(object = model_knn, data.test[,1:16], type="raw")

resultstest <- data.test[,17]
confusionMatrix(predictions,resultstest)


```

```{r importanceb2}

importance <- varImp(model_knn, scale=FALSE)
print(importance)
```

Plot bird 2
```{r scatterplotb2}

# plot scatter

p <- ggplot(birdsong2, aes(x = motif_duration, y = mean_entropy, color=contexts, alpha=0.5))
p <- p + geom_point()
p <- p +  theme_classic()
p

```


# Running stats to see other variables are significantly different between two contexts.

Mean pitch and mean entropy are two variables that ranked high for both birds. Running statistical tests to see if these mesures are different in different contexts.

# Mean pitch
```{r birdstats2}
# Run anova
anov <- aov(new_data$mean_pitch ~ new_data$contexts)
# output summary
summary(anov)
# Run tukey test for pairwise comparison
TukeyHSD(anov)
```
Mean pitch is differs with context for O375 but not for G544. 

# Mean entropy stats
```{r birdstats3}
# Run anova
anov <- aov(new_data$mean_entropy ~ new_data$contexts)
# output summary
summary(anov)
# Run tukey test for pairwise comparison
TukeyHSD(anov)
```
Mean entropy is differs with context for both birds. 



Just looking at how the accuracy changes with different k values.


```{r knnparameters}
set.seed(400)
ctrl <- trainControl(method="repeatedcv",repeats = 3) #,classProbs=TRUE,summaryFunction = twoClassSummary)
knnFit <- train(contexts ~ ., data = data.training, method = "knn", trControl = ctrl, preProcess = c("center","scale"), tuneLength = 20)

#Output of kNN fit
knnFit

```

Plot the result.


```{r knnparametersplot}

ggplot(knnFit, aes(x=k, y=Accuracy)) + ylim(0,1) + theme_classic()


```

It doesn't seem to matter which numbers we choose for neighbors.

## PCA (Principal component analysis)

Bird #1 (G544)
```{r PCA1}

bs2.pca <- prcomp(birdsong1[,2:12])


library(ggfortify)
pca.plot <- autoplot(bs2.pca, data = birdsong1, colour = 'contexts')
pca.plot

```



Bird #2 (O375)
```{r PCA}

bs2.pca <- prcomp(birdsong2[,2:12])

library(ggfortify)
pca.plot <- autoplot(bs2.pca, data = birdsong2, colour = 'context2')
pca.plot
```


## Exploring different plots


```{r PCAbird1_2}
library(FactoMineR)
library(factoextra)
new_birdsong1 <- birdsong1[,c(2:16,27:28)]
bird1.pca <- PCA(new_birdsong1[,-17], graph = FALSE)

fviz_pca_ind(bird1.pca,
             geom.ind = "point", # show points only (nbut not "text")
             col.ind = new_birdsong1$contexts, # color by groups
             palette = c("#00AFBB", "#E7B800"),
             addEllipses = TRUE, # Concentration ellipses
             legend.title = "Groups"
             )

```





```{r PCAbird2_2}
new_birdsong2 <- birdsong2[,c(2:16,27:28)]

bird2.pca <- PCA(new_birdsong2[,-17], graph = FALSE)

fviz_pca_ind(bird2.pca,
             geom.ind = "point", # show points only (nbut not "text")
             col.ind = new_birdsong2$contexts, # color by groups
             palette = c("#00AFBB", "#E7B800"),
             addEllipses = TRUE, # Concentration ellipses
             legend.title = "Groups"
)
```


## Conclusion 2

Although the songs that are recorded in different contexts are statistically different in song length, It is hard to distinguish song types by just comparing the audio parameters. 
For more accuate analysis, we will need more recordings of actual direct songs (verified through behavioral analysis on video), which is time consuming. 































